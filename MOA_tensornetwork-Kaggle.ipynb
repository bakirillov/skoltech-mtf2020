{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import tensorly as tl\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot\n",
    "from torch.optim import Adam\n",
    "from matplotlib import rcParams\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import tensorly as tl\n",
    "from tensorly.random import random_tucker\n",
    "from tensorly.tucker_tensor import tucker_to_tensor\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#tl.set_backend('pytorch')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import tensorly as tl\n",
    "from tensorly.random import random_tucker\n",
    "from tensorly.tucker_tensor import tucker_to_tensor\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#tl.set_backend('pytorch')\n",
    "\n",
    "\n",
    "class NeuralTensorLayer(torch.nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    This is the class for \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, order, input_dim, output_dim, rank_tucker=-1,\n",
    "                 initializer=torch.nn.init.xavier_uniform):\n",
    "        \n",
    "        super(NeuralTensorLayer, self).__init__()\n",
    "        \n",
    "        self.order = order\n",
    "        self.rank_tucker = rank_tucker\n",
    "        \n",
    "        if order > 3 or order < 1:\n",
    "            raise Exception('Order must be in range [1, 3]')\n",
    "            \n",
    "        if rank_tucker != -1 and rank_tucker < 1:\n",
    "            raise Exception('Tucker rank must be -1 or greater than 0 integer')\n",
    "            \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.zeros((1, output_dim)), requires_grad=True)\n",
    "        initializer(self.bias)\n",
    "        \n",
    "        self.myparameters = torch.nn.ParameterList([self.bias])\n",
    "        \n",
    "        self.order1_tens = self.initialize_n_order_tensor(1, initializer)\n",
    "        \n",
    "        if order >= 2:\n",
    "            self.order2_tens = self.initialize_n_order_tensor(2, initializer)\n",
    "            \n",
    "        if order == 3:\n",
    "            self.order3_tens = self.initialize_n_order_tensor(3, initializer)\n",
    "        \n",
    "    # initialize tensor in full or in decomposed form and register it as parameter\n",
    "    def initialize_n_order_tensor(self, order, initializer):\n",
    "        \n",
    "        if self.rank_tucker >= 1:\n",
    "            \n",
    "            dim_list = [self.input_dim] * order + [self.output_dim]\n",
    "            tens_core, factors = random_tucker(dim_list, self.rank_tucker)\n",
    "            tens_core = nn.Parameter(tens_core, requires_grad=True)\n",
    "            factors = [nn.Parameter(fact, requires_grad=True) for fact in factors]\n",
    "            \n",
    "            self.myparameters.append(tens_core)\n",
    "            for fact in factors:\n",
    "                self.myparameters.append(fact)\n",
    "                \n",
    "            return (tens_core, factors)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            dim_list = [self.input_dim] * order + [self.output_dim]\n",
    "            var = nn.Parameter(torch.zeros(dim_list), requires_grad=True)\n",
    "            initializer(var)\n",
    "            self.myparameters.append(var)\n",
    "            \n",
    "            return var\n",
    "\n",
    "    def compute_result_for_vec(self, core, factor_inp, last_factor): # result dim (1, 1)\n",
    "        result = core\n",
    "        for i in range(len(factor_inp)):\n",
    "            result = tl.tenalg.mode_dot(result, factor_inp[i], i)\n",
    "        result = result.view(1, -1).mm(torch.transpose(last_factor, 0, 1))\n",
    "        return result.view(-1)\n",
    "\n",
    "    def mode_n_dot_accelerated(self, core, factors, input):\n",
    "\n",
    "        new_factors = [torch.transpose(factors[i], 0, 1).mm(input) for i in range(len(factors) - 1)]\n",
    "\n",
    "        return torch.stack([\n",
    "                            self.compute_result_for_vec(core, \n",
    "                                                        [new_factors[k][:, i] for k in range(len(factors) - 1)], factors[-1]) \n",
    "                            for i in range(input.shape[1])\n",
    "                            ], dim=0)\n",
    "        \n",
    "    def forward(self, X, transposed=False):\n",
    "        \n",
    "        #X = torch.Tensor(X)\n",
    "        \n",
    "        if self.rank_tucker == -1:\n",
    "            result = torch.addmm(self.bias, X, self.order1_tens)\n",
    "        else:\n",
    "            result = torch.addmm(self.bias, X, tucker_to_tensor(self.order1_tens))\n",
    "        \n",
    "        if self.order >= 2:\n",
    "            \n",
    "            if self.rank_tucker == -1:      \n",
    "                acc = tl.tenalg.mode_dot(self.order2_tens, X, 0)\n",
    "            else:\n",
    "                acc = tl.tenalg.mode_dot(tucker_to_tensor(self.order2_tens), X, 0)\n",
    "\n",
    "            acc = tl.tenalg.mode_dot(acc, X, 1)\n",
    "            result += torch.einsum('iik->ik', acc)\n",
    "        \n",
    "        if self.order == 3:\n",
    "             \n",
    "            if self.rank_tucker == -1:      \n",
    "                acc = tl.tenalg.mode_dot(self.order3_tens, X, 0)\n",
    "            else:\n",
    "                acc = tl.tenalg.mode_dot(tucker_to_tensor(self.order3_tens), X, 0)\n",
    "            \n",
    "            acc = tl.tenalg.mode_dot(acc, X, 1)\n",
    "            acc = tl.tenalg.mode_dot(acc, X, 2)\n",
    "            result += torch.einsum('iiik->ik', acc)\n",
    "        \n",
    "        return tl.reshape(result, (X.shape[0], self.output_dim))\n",
    "\n",
    "    def get_orthogonality_loss(self):\n",
    "\n",
    "        if self.rank_tucker == -1:\n",
    "            return 0\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        for fact in self.order1_tens[1]:\n",
    "            loss += torch.sum((tl.dot(fact.T, fact) - torch.eye(fact.shape[1]).cuda()) ** 2)\n",
    "        \n",
    "        if self.order >= 2:\n",
    "            \n",
    "            for fact in self.order2_tens[1]:\n",
    "                loss += torch.sum((tl.dot(fact.T, fact) - torch.eye(fact.shape[1]).cuda()) ** 2)\n",
    "        \n",
    "        if self.order == 3:\n",
    "             \n",
    "            for fact in self.order3_tens[1]:\n",
    "                loss += torch.sum((tl.dot(fact.T, fact) - torch.eye(fact.shape[1]).cuda()) ** 2)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class MOA_set(Dataset):\n",
    "    \n",
    "    def __init__(self, fn_X, fn_Y=None, cuda=True):\n",
    "        self.X = np.load(fn_X)\n",
    "        self.Y = None if not fn_Y else np.load(fn_Y)\n",
    "        self.Yn = True if not fn_Y else False\n",
    "        self.cuda = cuda\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(self.X.shape[0])\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        x = torch.from_numpy(self.X[ind]).type(torch.FloatTensor)\n",
    "        if self.cuda:\n",
    "            x = x.cuda()\n",
    "        if not self.Yn:\n",
    "            y = torch.from_numpy(self.Y[ind]).type(torch.FloatTensor)\n",
    "            if self.cuda:\n",
    "                y = y.cuda()\n",
    "            return x, y\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#NN = torch.nn.Sequential(NeuralTensorLayer(2, input_dim, 2, rank_tucker=5), \n",
    " #                        nn.BatchNorm1d(2),\n",
    "  #                       nn.Softmax(dim=-1))\n",
    "#loss = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(NN.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "class MOA_set(Dataset):\n",
    "    \n",
    "    def __init__(self, fn_X, fn_Y=None, cuda=True):\n",
    "        self.X = np.load(fn_X)\n",
    "        self.Y = None if not fn_Y else np.load(fn_Y)\n",
    "        self.Yn = True if not fn_Y else False\n",
    "        self.cuda = cuda\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(self.X.shape[0])\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        x = torch.from_numpy(self.X[ind]).type(torch.FloatTensor)\n",
    "        if self.cuda:\n",
    "            x = x.cuda()\n",
    "        if not self.Yn:\n",
    "            y = torch.from_numpy(self.Y[ind]).type(torch.FloatTensor)\n",
    "            if self.cuda:\n",
    "                y = y.cuda()\n",
    "            return(x, y)\n",
    "        else:\n",
    "            return(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TensorNet(nn.Module): \n",
    "    \n",
    "    def __init__(\n",
    "        self, order, input_dim, output_dim, rank_tucker=5\n",
    "    ):\n",
    "        super(TensorNet, self).__init__()\n",
    "        self.tensor = NeuralTensorLayer(\n",
    "            order, input_dim, output_dim, rank_tucker=rank_tucker\n",
    "        )\n",
    "        self.bn = nn.BatchNorm1d(output_dim)\n",
    "        #self.l = nn.Linear(tensor_dim, output_dim)\n",
    "        self.s = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return(self.s(self.bn(self.tensor(x))))\n",
    "    \n",
    "    def predict(self, loader, loss=None, train=True, verbose=False, orth_alpha=None):\n",
    "        Y_hat = []\n",
    "        mean_loss = 0\n",
    "        ld = tqdm(loader) if verbose else loader\n",
    "        for i,a in enumerate(ld):\n",
    "            d = a[0] if train else a\n",
    "            c_Y_hat = self.forward(d)\n",
    "            c_y_hat = c_Y_hat.cpu().data.numpy()\n",
    "            Y_hat.append(c_y_hat)\n",
    "            if train:\n",
    "                loss_val = loss(c_Y_hat, a[1])\n",
    "                if orth_alpha:\n",
    "                    loss_val += self.tensor.get_orthogonality_loss()*orth_alpha\n",
    "                mean_loss += loss_val.cpu().data.numpy()\n",
    "        return(np.concatenate(Y_hat), mean_loss/i)\n",
    "    \n",
    "    def fit(\n",
    "        self, loader, loss, optimizer, scheduler, n_iter, \n",
    "        val_loader, metrics, print_every=10, orth_alpha=0.1, fit_orthogonality=True\n",
    "    ):\n",
    "        history = {a: [] for a in metrics}\n",
    "        history[\"train_loss\"] = []\n",
    "        history[\"val_loss\"] = []\n",
    "        for j in tqdm(np.arange(n_iter)):\n",
    "            mean_loss = 0\n",
    "            self.train()\n",
    "            for i,(batch_X, batch_Y) in enumerate(loader):\n",
    "                optimizer.zero_grad()\n",
    "                Y_hat = self.forward(batch_X)\n",
    "                loss_val = loss(Y_hat, batch_Y)\n",
    "                if fit_orthogonality:\n",
    "                    loss_val += self.tensor.get_orthogonality_loss()*orth_alpha\n",
    "                loss_val.backward()\n",
    "                optimizer.step()\n",
    "                mean_loss += loss_val.cpu().data.numpy()\n",
    "            history[\"train_loss\"].append(mean_loss/i)\n",
    "            self.eval()\n",
    "            val_Y_hat, val_loss = self.predict(val_loader, loss, orth_alpha=orth_alpha)\n",
    "            history[\"val_loss\"].append(val_loss)\n",
    "            val_Y = np.concatenate([a[1].cpu().data.numpy() for a in val_loader])\n",
    "            for m in metrics:\n",
    "                history[m].append(metrics[m](val_Y, val_Y_hat))\n",
    "            if (j+1) % print_every == 0:\n",
    "                print(\"epoch#\"+str(j))\n",
    "                print(\"train loss\", history[\"train_loss\"][-1])\n",
    "                print(\"val loss\", history[\"val_loss\"][-1])\n",
    "                for m in metrics:\n",
    "                    print(m, history[m][-1])\n",
    "            clip_grad_norm_(self.parameters(), 0.3)\n",
    "            scheduler.step()\n",
    "        return(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8d8009dbd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "torch.manual_seed(1377)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MOA_set(\"train_X.npy\", \"train_Y.npy\")\n",
    "val_set = MOA_set(\"val_X.npy\", \"val_Y.npy\")\n",
    "test_set = MOA_set(\"testing_X.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17860, 5954, 3982)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set), len(val_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bakirillov/anaconda3/envs/lapki/lib/python3.7/site-packages/ipykernel_launcher.py:57: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    }
   ],
   "source": [
    "model = TensorNet(2, 879, 206).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colwise_logloss(Y, Y_hat):\n",
    "    return(-(Y * np.log(Y_hat+10**-6) + (1 - Y) * np.log(1 - Y_hat+10**-6)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9054e87cc3d840cf99d6caf9bedca8a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch#4\n",
      "train loss 685.4187506125392\n",
      "val loss 464.7317954442834\n",
      "CWLL 0.12539543\n",
      "epoch#9\n",
      "train loss 27.923073686579222\n",
      "val loss 20.08381945087064\n",
      "CWLL 0.045494698\n",
      "epoch#14\n",
      "train loss 1.0983233428343222\n",
      "val loss 0.7346958380232576\n",
      "CWLL 0.026120508\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-536d54108d7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"CWLL\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcolwise_logloss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-2-49bf5125e31c>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, loader, loss, optimizer, scheduler, n_iter, val_loader, metrics, print_every, orth_alpha, fit_orthogonality)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfit_orthogonality\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                     \u001b[0mloss_val\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_orthogonality_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0morth_alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m                 \u001b[0mloss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-49bf5125e31c>\u001b[0m in \u001b[0;36mget_orthogonality_loss\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfact\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder1_tens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfact\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_loader, loss, optimizer, scheduler, 30, val_loader, \n",
    "    {\"CWLL\": colwise_logloss}, 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams[\"figure.figsize\"] = (10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2)\n",
    "axes[0].plot(history[\"train_loss\"])\n",
    "axes[0].plot(history[\"val_loss\"])\n",
    "axes[1].plot(history[\"CWLL\"])\n",
    "axes[0].set_ylabel(\"BCE\")\n",
    "axes[1].set_ylabel(\"CWLL\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.bin\", \"wb\") as oh:\n",
    "    torch.save(model, oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test_loader, train=False, verbose=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"data/test_features.csv\", index_col=\"sig_id\")\n",
    "targets = pd.read_csv(\"data/train_targets_scored.csv\", index_col=\"sig_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.columns = targets.columns\n",
    "out_df.index = test_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.to_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
